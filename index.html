<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mohammad Rezaei - Personal Page</title>
    <link rel="stylesheet" href="style.css"> </head>
<body>
    <header>
        <div class="hero">
            <h1>Mohammad Rezaei</h1>
            <p class="tagline">Where Creativity Meets Technology</p>
            <p>I'm a computer scientist with a keen interest in statistical modeling and deep learning, particularly their applications in NLP and computer vision.</p>
            <a href="#projects" class="button">View My Projects</a>
            </div>
    </header>

    <main>
        <section id="about">
            <h2>About Me</h2>
            <p>I'm a computer scientist with a deep-seated passion for understanding and leveraging the power of data. My primary interests lie in the fascinating realms of statistical modeling and deep learning, where I'm particularly drawn to their transformative applications in Natural Language Processing and Computer Vision. I'm motivated by the potential of these technologies to solve complex real-world problems and build intelligent systems that can understand and interact with the world around us.</p>
            </section>

        <section id="projects">
            <h2>Projects</h2>
            <div class="project-grid">

                <div class="project-item">
                    <h3>Building a Scalable Face Identification and retrieval System on Google Cloud Platform </h3>
                    <img src="images/faceid.png" alt="Screenshot of client">
                    <p> This project implements a scalable person identification service on Google Cloud Platform. It Utilizes DeepFace for facial embedding extraction and Vertex AI Vector Search for rapid similarity searches across image collections stored in GCS. Features a Streamlit client interacting with a Cloud Run-deployed server API.</p>
                    <p><strong>Technologies Used:</strong> Python, Google Cloud Platform, Vertex AI Vector Search, Steamlit, FastAPI, Streamlit, DeepFace, Tensorflow</p>
                    <div class="project-links">
                        <a href="https://github.com/mrezaei92/person-id-gcp" target="_blank">Github</a>
                    </div>

                </div>

                
                <div class="project-item">
                    <h3>TriHorn-Net</h3>
                    <img src="images/trihornnet.jpeg" alt="Screenshot of TriHorn-Net">
                    <p><strong>TriHorn-Net:</strong> I proposed and implemented a novel two-stage network architecture for hand pose estimation from depth images. This network, called TriHorn-Net, utilizes a unique combination of three attention-based branches to accurately predict joint positions. My approach, which includes a constrained UV branch and an unconstrained attention enhancement branch fused to guide feature pooling, achieves state-of-the-art performance in the field.</p>
                    <p><strong>Technologies Used:</strong> Python, Pytorch, Deep Learning</p>
                    <div class="project-links">
                        <a href="https://github.com/mrezaei92/TriHorn-Net" target="_blank">Github</a>
                        <span style="margin-left: 10px; margin-right: 10px;">|</span> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417423004232" target="_blank">Paper</a>
                    </div>

                </div>

                <div class="project-item">
                    <h3>Weakly-supervised Hand Part Segmentation from Depth Images</h3>
                    <img src="images/handseg.png" alt="Screenshot of Handseg">
                    <p>This project addresses the challenge of requiring large amounts of labeled data for accurate part segmentation by proposing a novel data-driven method for hand part segmentation on depth maps. The key innovation is that it avoids the need for explicit segmentation labels by leveraging readily available 3D hand joint locations from existing datasets. The method learns to estimate hand shape and pose from depth maps, generates a 3D hand mesh, renders it using LBS weights, and derives segmentation labels from the rendered image. Evaluated on a manually annotated subset of the NYU dataset, the proposed approach achieves a mIoU of 42% without using any segmentation-based labels during training, demonstrating its effectiveness. </p>
                    <p><strong>Technologies Used:</strong> Python, Pytorch, Deep Learning</p>
                    <div class="project-links">
                        <a href="https://github.com/mrezaei92/HandPartSegment" target="_blank">GitHub</a>
                        <span style="margin-left: 10px; margin-right: 10px;">|</span> <a href="https://dl.acm.org/doi/10.1145/3453892.3453902" target="_blank">Paper</a>

                    </div>
                </div>

                 <div class="project-item">
                    <h3>Semi-Supervised 3d Hand Pose Estimation with Consistency Training</h3>
                    <img src="images/semihand.png" alt="Screenshot of Semihand">
                    <p>This project tackled the challenge of limited labeled data in 3D hand pose estimation from depth images. I implemented a semi-supervised deep learning method featuring a novel teacher-student network architecture. The teacher network learns from both labeled and unlabeled data by encouraging consistent pose estimations under various transformations. The student network then learns from the teacher's predictions on unlabeled data. This approach significantly reduces the need for extensive labeled datasets and achieves state-of-the-art performance compared to other semi-supervised methods. </p>
                    <p><strong>Technologies Used:</strong> Python, Pytorch, Deep Learning</p>
                    <div class="project-links">
                        <a href="https://github.com/mrezaei92/semi_hand" target="_blank">GitHub</a>
                        <span style="margin-left: 10px; margin-right: 10px;">|</span> <a href="https://arxiv.org/abs/2303.15147" target="_blank">Paper</a>

                    </div>
                </div>

                <div class="project-item">
                    <h3>Deep Q-Learning Driven Reinforcement Learning for Autonomous Gameplay in Super Mario Bros</h3>
                    <img src="images/playback.gif" alt="Screenshot of Super Mario">
                    <p>In this project, I explored the exciting field of Deep Reinforcement Learning by training an AI agent to play the classic Nintendo game Super Mario Bros. (1985). Using Deep Q-Learning, the agent learned to control Mario directly from raw pixel data â€“ just like a human player would see the screen. The OpenAI Gym environment provided the platform for the agent to interact with the game and learn optimal strategies through trial and error. </p>
                    <p><strong>Technologies Used:</strong> Python, Pytorch, Deep Learning, OpenAI Gym</p>
                    <div class="project-links">
                        <a href="https://github.com/mrezaei92/SuperMario_DeepQlearning" target="_blank">GitHub</a>

                    </div>
                </div>


                

                </div>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>Feel free to reach out!</p>
            <p><strong>Email:</strong> mohammad.rezaei92@gmail.com</p>
            </section>
    </main>

    <footer>
        <p>&copy; 2025 Mohammad Rezaei</p>
        </footer>

    </body>
</html>
